# Custom Checks Implementation - Graceful Shutdown

## Overview

Implemented full support for `custom_checks` in the dynamic evaluation system, including the graceful shutdown test for task-03.

---

## Changes Made

### 1. Evaluator Lambda (`evaluation/lambda/evaluator_dynamic.py`)

#### Added Methods:

**`run_custom_checks()`** (line 512-526)
- Iterates through `custom_checks` defined in task spec
- Routes each check to appropriate handler
- Currently handles `graceful_shutdown`
- Extensible for future custom checks

**`check_graceful_shutdown()`** (line 528-581)
- Tests if frontend calls backend `/game-over` on termination
- Steps:
  1. Get backend pod and initial logs
  2. Count initial `/game-over` calls
  3. Get frontend pod
  4. Delete frontend pod (triggers preStop hook)
  5. Wait 15 seconds for termination
  6. Check backend logs again
  7. Verify `/game-over` count increased
- Returns `True` if working, `False` otherwise

**`get_pod_by_label()`** (line 583-599)
- Helper to find pod by label selector
- Returns first matching pod

**`delete_pod()`** (line 601-607)
- Helper to delete a pod
- Returns success status

#### Updated Methods:

**`evaluate()`** (line 200-220)
- Added call to `run_custom_checks()` after application checks
- Maintains evaluation flow consistency

---

### 2. Task-03 Spec (`tasks/task-03/task-spec.yaml`)

#### Fixed Scoring Inconsistency:

```yaml
# Before:
custom_checks:
  - check_id: "graceful_shutdown"
    points: 20  # âŒ Inconsistent

scoring:
  criteria:
    - id: "graceful_shutdown"
      points: 5  # Different value!

# After:
custom_checks:
  - check_id: "graceful_shutdown"
    points: 5  # âœ… Consistent

scoring:
  criteria:
    - id: "graceful_shutdown"
      points: 5  # Matches!
```

---

## How It Works

### Graceful Shutdown Test Flow:

```
1. Before Test
   â”œâ”€ Backend pod running
   â”œâ”€ Frontend pod running
   â””â”€ Backend logs: 0 /game-over calls

2. Evaluator Actions
   â”œâ”€ Read initial backend logs
   â”œâ”€ Count /game-over occurrences: 0
   â”œâ”€ Delete frontend pod
   â””â”€ Wait 15 seconds

3. What Happens
   â”œâ”€ Kubernetes sends SIGTERM to frontend
   â”œâ”€ Frontend preStop hook executes:
   â”‚  â””â”€ Calls: POST http://svc-backend.task-03.svc.cluster.local:5000/game-over
   â”œâ”€ Backend receives request, logs it
   â”œâ”€ Frontend terminates
   â””â”€ New frontend pod starts

4. After Test
   â”œâ”€ Evaluator reads backend logs again
   â”œâ”€ Count /game-over occurrences: 1+
   â””â”€ Test passes if count increased
```

---

## Testing the Implementation

### Before Update (Current Behavior):

```bash
~/student-tools/request-evaluation.sh task-03
# Result:
{
  "score": 100,
  "graceful_shutdown": false  # âŒ Always false
}
```

### After Update (Expected Behavior):

```bash
~/student-tools/request-evaluation.sh task-03
# Result:
{
  "score": 100,
  "graceful_shutdown": true   # âœ… Actually tested!
}
```

---

## Deployment Instructions

### Step 1: Upload Updated Task-03 Spec

From PowerShell in AWS Learner Lab:

```powershell
cd path\to\k8s-assessment-framework
aws s3 cp tasks\task-03\task-spec.yaml s3://k8s-eval-results/task-specs/task-03/task-spec.yaml
```

### Step 2: Deploy Updated Lambda

**Option A - AWS CLI:**

```powershell
cd path\to\k8s-assessment-framework
aws lambda update-function-code `
  --function-name k8s-evaluator `
  --zip-file fileb://evaluation/lambda/lambda-deployment.zip
```

**Option B - AWS Console:**

1. Open: https://console.aws.amazon.com/lambda
2. Find function: `k8s-evaluator`
3. Click **"Upload from"** > **".zip file"**
4. Select: `evaluation/lambda/lambda-deployment.zip`
5. Click **"Save"**

### Step 3: Test the Update

```bash
# SSH to student instance
ssh ubuntu@ip-10-0-1-148

# Request evaluation
~/student-tools/request-evaluation.sh task-03
```

**Expected Output:**

```json
{
  "score": 100,
  "max_score": 100,
  "results_summary": {
    "deployment_backend_exists": true,
    "deployment_frontend_exists": true,
    "service_svc-backend_exists": true,
    "service_svc-frontend_exists": true,
    "backend_get_config": true,
    "backend_ping": true,
    "frontend_startup": true,
    "frontend_who_am_i": true,
    "frontend_health": true,
    "startup_probe_configured": true,
    "liveness_probe_configured": true,
    "graceful_shutdown": true  // âœ… NOW TRUE!
  }
}
```

---

## Extending Custom Checks

The implementation is designed to be extensible. To add new custom checks:

### 1. Define in Task Spec:

```yaml
custom_checks:
  - check_id: "my_custom_check"
    description: "Description of what it tests"
    points: 10
    validation_steps:
      - step1
      - step2
```

### 2. Add to Scoring Criteria:

```yaml
scoring:
  criteria:
    - id: "my_custom_check"
      description: "My custom check"
      points: 10
```

### 3. Implement in Evaluator:

```python
def run_custom_checks(self):
    custom_checks = self.task_spec.get('custom_checks', [])

    for check in custom_checks:
        check_id = check['check_id']

        if check_id == 'graceful_shutdown':
            self.results[check_id] = self.check_graceful_shutdown(check)
        elif check_id == 'my_custom_check':
            self.results[check_id] = self.check_my_custom_check(check)
        else:
            print(f"Warning: Unknown custom check: {check_id}")
            self.results[check_id] = False

def check_my_custom_check(self, check):
    # Implement custom logic here
    try:
        # Your test logic
        return True  # or False
    except Exception as e:
        print(f"Error: {e}")
        return False
```

---

## Architecture Benefits

### Dynamic Evaluation
- âœ… Task specs define what to test
- âœ… Evaluator automatically runs all checks
- âœ… No hardcoded task logic

### Extensibility
- âœ… Easy to add new custom checks
- âœ… Each task can have unique tests
- âœ… Reusable check implementations

### Transparency
- âœ… Students see exact test results
- âœ… Clear pass/fail criteria
- âœ… Debugging friendly

---

## Files Modified

1. âœ… `/evaluation/lambda/evaluator_dynamic.py` - Added custom checks support
2. âœ… `/tasks/task-03/task-spec.yaml` - Fixed scoring inconsistency
3. âœ… `/evaluation/lambda/lambda-deployment.zip` - Deployment package created
4. âœ… `/evaluation/lambda/deploy-lambda.sh` - Deployment script

---

## Summary

**Before:**
- `custom_checks` section ignored
- Graceful shutdown always showed `false`
- Got 100/100 anyway (those 5 points not counted)

**After:**
- `custom_checks` fully implemented
- Graceful shutdown actually tested
- True dynamic evaluation achieved!

**Impact:**
- âœ… True dynamic evaluation
- âœ… Every task spec element is evaluated
- âœ… Extensible for future custom checks
- âœ… Task-03 fully functional

---

## Next Steps

1. **Deploy Lambda update** (see instructions above)
2. **Upload task-03 spec to S3**
3. **Test task-03 evaluation**
4. **Verify graceful_shutdown: true**

Your assessment framework is now truly dynamic! ðŸš€
